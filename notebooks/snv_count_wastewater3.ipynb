{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count snvs in wastewater samples from ShoRAH output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import gzip\n",
    "import csv\n",
    "import strictyaml\n",
    "from Bio import SeqIO\n",
    "from BCBio import GFF\n",
    "import subprocess\n",
    "from IPython.core.display import display, HTML\n",
    "from termcolor import colored\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import pysam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals\n",
    "\n",
    "A few general variable about where to find stuff. Adapt to your own needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "voc_dir = '../voc/' # where COJAC stores its variants' YAMLs\n",
    "vpipe_working = 'working' # V-pipe's working directory\n",
    "ww_samples_tsv = f\"{vpipe_working}/samples.wastewateronly.tsv\" # samples TSV file listing the waste water samples\n",
    "genes_gff = f\"{vpipe_working}/references/gffs/Genes_NC_045512.2.GFF3\" # genes table\n",
    "\n",
    "# Outputs\n",
    "muttable_tsv='mutlist.txt'\n",
    "tables_dir='snv_tables'\n",
    "if not os.path.isdir(tables_dir):\n",
    "    try:\n",
    "        os.mkdir(tables_dir, mode=0o775)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define mutations to look for\n",
    "\n",
    "This is now done by parsing the variant's YAML file and the mutation position listed therein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxmutdec=re.compile('^(?:(?:(?:(?P<ref>[ATCG]+)\\>)?(?P<mut>[ATCG]+))|(?P<del>[\\-]+)|(?:[\\+](?P<ins>[ATGC]+)))$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartable = pd.DataFrame(data={'position': [], 'reference': [], 'variant': []}).astype({'position':'int'})\n",
    "for yp in glob.glob(os.path.join(voc_dir,'*.yaml')): # auto skips .hidden\n",
    "    with open(yp, 'r') as yf:\n",
    "        yam = strictyaml.dirty_load(yf.read(), allow_flow_style=True).data\n",
    "    muts = pd.DataFrame(data={'position': [], 'reference': [], 'variant': [], yam['variant']['short']:[]}).astype({'position':'int'})\n",
    "    for c in ['mut','extra','shared','subset']:\n",
    "        # all categories (we don't care, we will compare accross samples)\n",
    "        if c in yam:\n",
    "            for pos,mutstr in yam[c].items():\n",
    "                if not (res:=rxmutdec.match(mutstr)):\n",
    "                    print(f\"{yp}:{pos} cannot parse {mutstr}\")\n",
    "                    continue\n",
    "                match=res.groupdict()\n",
    "                if match['ins']:\n",
    "                    print(f\"{yp}:{pos} insertions not supported (yet): {match['ins']}\")\n",
    "                    continue\n",
    "                elif match['mut']:\n",
    "                    for i in range(len(match['mut'])):\n",
    "                        muts=muts.append([{'position':int(pos)+i, 'reference':(match['ref'][i] if match['ref'] and i<len(match['ref']) else 'N'), 'variant':match['mut'][i], yam['variant']['short']:c}])\n",
    "                elif match['del']:\n",
    "                    # TODO this is wrong and will be fixed in ShoRAH\n",
    "                    for i in range(len(match['del'])):\n",
    "                        muts=muts.append([{'position':int(pos)+i, 'reference':(match['ref'][i] if match['ref'] and i<len(match['ref']) else 'N'), 'variant':'-', yam['variant']['short']:c}])\n",
    "\n",
    "    vartable = vartable.merge(how='outer', right=muts, copy=False, sort=True)#.fillna('')\n",
    "with pd.option_context('display.max_rows', None): #, 'display.max_columns', None):\n",
    "    display(vartable) #.sort_values('position'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"gene\" in vartable.columns:\n",
    "    vartable.insert(3, \"gene\", [''] * len(vartable.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartable[\"gene\"]=''\n",
    "if genes_gff:\n",
    "    with open(genes_gff) as gf:\n",
    "        for record in GFF.parse(gf):\n",
    "            for feature in record.features:\n",
    "                if feature.type == 'gene': \n",
    "                    mask=(int(feature.location.end) >= vartable[\"position\"]) & (vartable[\"position\"]  >= int(feature.location.start))\n",
    "                    vartable.loc[mask,'gene']=feature.qualifiers.get('Name', [feature.id])[0]\n",
    "display(vartable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vartable.to_csv('mutlist.txt',sep='\\t', index=False, na_rep=\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_range(filename):\n",
    "    '''extract the window range from a shorah window filename:'''\n",
    "    match = re.search('([0-9]+)\\-([0-9]+).reads', filename)\n",
    "    return (int(match.group(1)), int(match.group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_snvs(filename, shorah_table):\n",
    "    '''Function to produce a n_local_haplot X p_snv_falling_in_the_local_haplo_window table of snv counts\n",
    "    Parameters:\n",
    "        filename: str of the name of the fasta.gz file for the shorah window\n",
    "        shorah_table: table outputted by shorah containing positions and \n",
    "    Return:\n",
    "        df_out: pd.DataFrame of snv counts with local haplos in the rows and snv's in the columns\n",
    "    '''\n",
    "    # extract range of window from filename\n",
    "    seqstart, seqstop = extract_range(filename)\n",
    "    # subset rows of shorah table for snv's falling in that range\n",
    "    shorah_table_subset = shorah_table[(shorah_table[\"position\"] >= seqstart) & (shorah_table[\"position\"] <= seqstop)]\n",
    "    # stop there and return None if no snv's fall in that range\n",
    "    if shorah_table_subset.shape[0] == 0:\n",
    "        return None\n",
    "    else:\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            window_lst = [] \n",
    "            window_names = []\n",
    "            # iterate through local haplos \n",
    "            for record in SeqIO.parse(f, \"fasta\"):\n",
    "                # keep seq name\n",
    "                window_names.append(record.description)\n",
    "                snv_lst = []\n",
    "                # iterate through snvs falling in the window\n",
    "                for i in range(shorah_table_subset.shape[0]):\n",
    "                    # test if the snv is present in this local haplo\n",
    "                    snv_lst.append((record.seq[shorah_table_subset[\"position\"].values[i]-seqstart] == \n",
    "                                    shorah_table_subset[\"variant\"].values[i]))\n",
    "                window_lst.append(snv_lst)    \n",
    "                \n",
    "        haplos_array = np.array(window_lst) * 1\n",
    "        snv_names = shorah_table_subset[\"reference\"] + \\\n",
    "            shorah_table_subset[\"position\"].astype('str') + \\\n",
    "            shorah_table_subset[\"variant\"]\n",
    "        df_out = pd.DataFrame(haplos_array, columns=snv_names, index=window_names)\n",
    "            \n",
    "        return df_out   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all wastewater samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ww_samples_tsv,'rt',encoding='utf-8', newline='') as tf:\t# this file has the same content as the original experiment\n",
    "    ww_sampledirs = [f\"working/samples/{sample}/{batch}/variants/SNVs/REGION_1/support/\" for (sample,batch,*r) in csv.reader(tf, delimiter='\\t')]\n",
    "ww_sampledirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dirlist = [ww_sampledirs[0] + i for i in os.listdir(ww_sampledirs[0])]\n",
    "temp_dirlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vartable[\"helo\"]\n",
    "except KeyError:\n",
    "    print(\"NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do it for one mutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_posterior = 0.8\n",
    "mut_number = 0\n",
    "temp_dirlist = [ww_sampledirs[0] + i for i in os.listdir(ww_sampledirs[0])]\n",
    "\n",
    "# find all snv tables for one mutation\n",
    "tmp_snvcounts = []\n",
    "mut_name = vartable.iloc[mut_number][\"reference\"] + \\\n",
    "    str(vartable.iloc[mut_number][\"position\"]) + \\\n",
    "    vartable.iloc[mut_number][\"variant\"]\n",
    "candidate_windows = 0 # keep track of candidate windows\n",
    "for win in temp_dirlist:\n",
    "    strt, stp = extract_range(win)\n",
    "    if strt <= vartable[\"position\"][mut_number] <= stp:\n",
    "        candidate_windows += 1\n",
    "        try:\n",
    "            snv_tab = count_snvs(win, vartable)[mut_name]\n",
    "        except KeyError:\n",
    "            snv_tab = None\n",
    "        if snv_tab is not None:\n",
    "            tmp_snvcounts.append(snv_tab)\n",
    "\n",
    "# sum haplos in each window and take the average\n",
    "ave_reads_full_lst = []\n",
    "for win in range(len(tmp_snvcounts)):\n",
    "    ave_reads_lst = []\n",
    "    for haplo in range(tmp_snvcounts[win].shape[0]):\n",
    "        haplo_name = tmp_snvcounts[win].index[haplo]\n",
    "        posterior = float(re.search(\"posterior=([0-1][\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "        ave_reads = float(re.search(\"ave_reads=([0-9]+[\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "        if posterior > min_posterior:\n",
    "            if tmp_snvcounts[win][haplo] == 1:\n",
    "                ave_reads_lst.append(ave_reads)\n",
    "    ave_reads_tmp = sum(ave_reads_lst)\n",
    "    ave_reads_full_lst.append(ave_reads_tmp)\n",
    "effective_windows = len(ave_reads_full_lst)\n",
    "ave_r = np.average(ave_reads_full_lst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_one_mut(temp_dirlist, vartable, mut_number, min_posterior=0.9):\n",
    "    '''Look for mutation number (mut_number) of (vartable) in (temp_dirlist)'''\n",
    "    # find all snv tables for one mutation\n",
    "    tmp_snvcounts = []\n",
    "    mut_name = vartable.iloc[mut_number][\"reference\"] + \\\n",
    "        str(vartable.iloc[mut_number][\"position\"]) + \\\n",
    "        vartable.iloc[mut_number][\"variant\"]\n",
    "    candidate_windows = 0 # keep track of candidate windows\n",
    "    for win in temp_dirlist:\n",
    "        strt, stp = extract_range(win)\n",
    "        if strt <= vartable[\"position\"][mut_number] <= stp:\n",
    "            candidate_windows += 1\n",
    "            try:\n",
    "                snv_tab = count_snvs(win, vartable)[mut_name]\n",
    "            except KeyError:\n",
    "                snv_tab = None\n",
    "            if snv_tab is not None:\n",
    "                tmp_snvcounts.append(snv_tab)\n",
    "\n",
    "    # sum haplos in each window and take the average\n",
    "    ave_reads_full_lst = []\n",
    "    for win in range(len(tmp_snvcounts)):\n",
    "        ave_reads_lst = []\n",
    "        for haplo in range(tmp_snvcounts[win].shape[0]):\n",
    "            haplo_name = tmp_snvcounts[win].index[haplo]\n",
    "            posterior = float(re.search(\"posterior=([0-1][\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "            ave_reads = float(re.search(\"ave_reads=([0-9]+[\\.]{0,1}[0-9]{0,})\", haplo_name).group(1))\n",
    "            if posterior > min_posterior:\n",
    "                if tmp_snvcounts[win][haplo] == 1:\n",
    "                    ave_reads_lst.append(ave_reads)\n",
    "        ave_reads_tmp = sum(ave_reads_lst)\n",
    "        ave_reads_full_lst.append(ave_reads_tmp)\n",
    "    effective_windows = sum([i>0 for i in ave_reads_full_lst])\n",
    "\n",
    "    # compute average \n",
    "    ave_r = np.average(ave_reads_full_lst) if len(ave_reads_full_lst) else 0\n",
    "    if not len(ave_reads_full_lst):\n",
    "        warnname=os.sep.join(str(temp_dirlist[0]).split(os.sep)[:-2])\n",
    "        print(f\"Warning! Can't average in {warnname}\")\n",
    "\n",
    "    return (candidate_windows, effective_windows, ave_r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_mut(temp_dirlist, vartable, min_posterior=0.9):\n",
    "    arr1 = np.array([list(search_one_mut(temp_dirlist, vartable, i, min_posterior)) for i in range(vartable.shape[0])])\n",
    "    temp_df = pd.DataFrame(arr1, columns=[\"candidate_windows\", \"effective_windows\", \"ave_reads\"])\n",
    "    temp_df = pd.concat([vartable, temp_df], axis=1)\n",
    "    return temp_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make all mutations outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mut_dfs=[]\n",
    "for sample in tqdm(ww_sampledirs):\n",
    "    # check if ShoRAH did output windows there\n",
    "    if not os.path.isdir(sample):\n",
    "        print(f\"Warning! No windows in {sample}!!!\")\n",
    "        continue\n",
    "\n",
    "    temp_dirlist = [sample + i for i in os.listdir(sample)]\n",
    "    if 0 == len(temp_dirlist):\n",
    "        print(f\"Warning! No windows in {sample}!!!\")\n",
    "        continue\n",
    "\n",
    "    mut_df = search_all_mut(temp_dirlist, vartable, min_posterior=0.9)\n",
    "    all_mut_dfs.append(mut_df)\n",
    "    spl=sample.split(os.sep)\n",
    "    mut_df.to_csv(os.path.join(tables_dir, f\"{spl[2]}-{spl[3]}_snv.csv\"), na_rep=\"NA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double checking code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww_sampledirs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dfnum in range(len(all_mut_dfs)):\n",
    "    target_dir = ww_sampledirs[dfnum]\n",
    "    for i in range(all_mut_dfs[dfnum].shape[0]):\n",
    "        if pd.isna(all_mut_dfs[dfnum][\"ave_reads\"][i]):\n",
    "            pos_to_check = all_dfs[dfnum][\"position\"][i]\n",
    "            print(pos_to_check)\n",
    "            lst1 = [ww_sampledirs[dfnum] + i for i in os.listdir(ww_sampledirs[dfnum])]\n",
    "            for d in lst1:\n",
    "                strt, stop = extract_range(d)\n",
    "                if strt <= pos_to_check <= stop:\n",
    "                    print(\"PROBLEM\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio1",
   "language": "python",
   "name": "bio1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
